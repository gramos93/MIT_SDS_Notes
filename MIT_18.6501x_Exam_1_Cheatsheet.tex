\documentclass[9pt,landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{enumitem}
\usepackage{bbm}

\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\title{MIT 18.650x Mid-Term 1 Cheatsheet}

\begin{document}

\raggedright
\footnotesize

\begin{center}
    \textbf{MIT 18.650x Mid-Term 1 Cheatsheet}\\
\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{1pt}

\section{Unit 2 - Parametric Estimation and Confidence Intervals}
In this unit, we introduce a mathematical formalization of statistical modeling to make a principled sense of the \underline{Trinity of Statistical} inference.

\begin{enumerate}
    \item {Estimation: $\hat{p} = \overline{R_n}$ is an estimator for the real parameter $p^*$.}
    \item {Confidence intervals: $[0.56, 0.73]$ is a 95\% confidence interval for $p$.}
    \item {Hypothesis testing: "We found \textit{statistical evidence} that more couples turn their head to the right."}
\end{enumerate}

\subsection{Chapter 4 - Inequalities}
Inequalities are used to bound quantities that might otherwise be hard to compute. \\

\textbf{Markov's inequality:} let $X$ be a non-negative r.v. and suppose $\mathbb{E}(X)$ exists. For any $t > 0$,
$$\mathbb{P}(X > t) = \frac{\mathbb{E}[X]}{t}$$

\textbf{Chebyshev's inequality:} let $\mu = \mathbb{E}[X]$ and $\sigma^2 = Var(X)$. Then, $$\mathbb{P}(|X - \mu| \ge t) \leq \frac{\sigma^2}{t^2}$$

\textbf{Hoeffding's inequality:} let $X_1, \dots, X_n \sim{} Ber(p)$ then for any $\epsilon > 0$, $$\mathbb{P}(|\overline{X_n} - p| > \epsilon) \leq 2e^{\frac{-2n\epsilon^2}{(b-a)^2}}$$
Where $\overline{X_n}$ is the sample average and $X \in [a, b]$.\\

\textbf{Jensen's inequality:} If $g$ is convex, then $$\mathbb{E}[g(X)] \ge g(\mathbb{E}[X])$$
The reverse is true if $g$ is concave.

\subsection{Chapter 5 - Convergence of r.v.'s}
There are 2 main ideas in this chapter. First, the \textbf{LLN} says that the \textit{sample average} converges in probability to $\mathbb{E}$. Second, the \textbf{CTL} says that
$\sqrt{n}(\overline{X_n} - \mu)$ converges in distribution to a Normal distribution for large enough $n$.\\
Let $X_1, \dots, X_n$ be a sequence of r.v's, $X$ is another r.v. Let $F_n(t)$ denote the CDF of $X_n$ an $F(t)$ denote the CDF of $X$.\\

\textbf{Convergence in probability:} $X_n \overset{P}{\to} X$ if, for every $\epsilon > 0$, $$\mathbb{P}(|X_n - X| > \epsilon) \to 0$$ as $n \to \infty$. 
\textbf{Warning !} One might think that $X_n \overset{P}{\to} b$ implies $\mathbb{E}[X] \to b$. This is \textbf{not} true. 

\textbf{Convergence in distribution:} $X_n \overset{d}{\to} X$ if, $$\lim_{n \to \infty} F_n(t) = F(t)$$ for all $t$ for which $F$ is continuous. 

\subsubsection{Properties of convergence}
The following relationships hold:
\begin{enumerate}[itemsep=0em]
    \item{$X_n \overset{P}{\to} X$ implies that $X_n \overset{d}{\to} X$.}
    \item{If $X_n \overset{d}{\to} X$ and if $\mathbb{P}(X=c) = 1$ for some real number $c$, then $X_n \overset{P}{\to} X$.}
    \item{If $X_n \overset{P}{\to} X$ and $Y_n \overset{P}{\to} Y$ then, $X_n + Y_n \overset{P}{\to} X+Y$.}
    \item{If $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} c$ then, $X_n + Y_n \overset{c}{\to} X + c$.}
    \item{If $X_n \overset{P}{\to} X$ and $Y_n \overset{P}{\to} Y$ then, $X_n Y_n \overset{P}{\to} XY$.}
    \item{If $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} c$ then, $X_n Y_n \overset{d}{\to} cX$.}
    \item{If $X_n \overset{P}{\to} X$ then, $g(X_n) \overset{P}{\to} g(X)$.}
    \item{If $X_n \overset{d}{\to} X$ then, $g(X_n) \overset{d}{\to} g(X)$.}
\end{enumerate}

Parts (4) and (6) are known as the \textbf{Slutzky's theorem}. Note that $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} Y$ does not imply that $X_n = Y_n \overset{d}{\to} X+Y$.

\subsubsection{Weak LLN \& CTL}
\textbf{Interpretation of the WLLN:}The distribution of $\overline{X_n}$ becomes more concentrated around $\mu$ as $n$ get larger. From the Chebyshev's inequality: 
$\mathbb{P}(|\overline{X_n} - \mu| > \epsilon) \leq \frac{Var(\overline{X_n})}{\epsilon^2} = \frac{\sigma^2}{n \epsilon^2}.$

\textbf{Central Limit Theorem:}let $X_1, \dots, X_n$ be i.i.d with mean $\mu$ and variance $\sigma^2$. Then 
$$Z_n \equiv \frac{\overline{X_n} - \mu}{\sqrt{Var(\overline{X_n})}} = \frac{\sqrt{n} (\overline{X_n} - \mu)}{\sigma} \overset{(d)}{\to} Z$$
where $Z$ is a standard Normal.\\
\textbf{Multivariate} version of the CTL. Let $X_1, \dots, X_n$ be i.i.d random \textit{vectors}, $X_i = [X_{1}, X_{2}, \dots, X_{k}]^T$ with mean $\mu = [\mu_{1}, \mu_{2}, \dots, \mu_{k}]^T$
and covariance matrix $\Sigma$. Then,$$\sqrt{n}(\overline{X} - \mu) \overset{(d)}{\to} N(0, \Sigma)$$

\subsubsection{The Delta Method}
The Delta Method allows us to find the limiting distribution of $g(Y_n)$ where $g$ is a \underline{smooth} function.
Suppose that $\frac{\sqrt{n}(Y_n - \mu)}{\sigma} \overset{(d)}{\to} N(0, 1)$ and $g$ is a \underline{differentiable} function such that $g'(\mu) \ne 0$. Then,

$$\sqrt{n}(g(Y_n) - g(\mu)) \overset{(d)}{\to} N(0, g'(\mu)|^2 \sigma^2)$$

\textbf{Multivariate} version of the Delta method. Let $Y_n = (Y_{n1}, \dots, Y_{nk})$ be a sequence random \textit{vectors} such that $\sqrt{n}(Y_n - \mu) \overset{(d)}{\to} N(0, \Sigma)$.
In this case $g: \mathbb{R}^k \to \mathbb{R}.$ Then,

$$\sqrt{n}(g(Y_n) - g(\mu)) \overset{(d)}{\to} N(0, \nabla g(\mu)^T \Sigma \nabla g(\mu))$$

\subsection{Chapter 6 Statistical Inference}
\textit{"\dots is the process of using data to infer the distribution that generated that data."}\\

\subsubsection{Point estimation: } 

Provides a \textit{best guess} of some quantity of interest. The point estimate of $\theta$, a fixed number, is $\hat{\theta}$, 
which depends on the data, therefore a r.v. A point estimator $\hat{\theta}_n$ of the parameter $\theta$ is \textbf{consistent} if $\hat{\theta}_n \overset{P}{\to} \theta$.
The \textbf{bias} of an estimator is defined as $bias(\hat{\theta}_n) = \mathbb{E_{\theta}}[\hat{\theta}_n] - \theta$. An estimator is unbiased if $\mathbb{E_{\theta}}[\hat{\theta}_n] = 0$.
Following the bias, the \textbf{mean squared error} (MSE) is $MSE = bias^2(\hat{\theta}_n) + \mathbb{V_{\theta}}(\hat{\theta}_n)$.\\

\subsubsection{Confidence Intervals: } 
Is an interval $I = (a, b)$ such that $\mathbb{P}_{\theta}(\theta \in I) \ge 1-\alpha$, for all $\theta \in \Theta$. 
$I$ is random, but $\theta$ is not random. Suppose that $\widehat{\theta}_n \approx N(\theta, \sigma^2)$. The confidence interval will be of the form 
$$I = \left [\widehat{\theta}_n - q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}}, \widehat{\theta}_n + q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}} \right ]$$

However, most of the time, this confidence interval will depend on the true parameter $\theta$. There are 3 methods to fix this problem.\\

\textbf{Conservative Bound:} Suppose we're calculating a confidence interval for the parameter $p$ of a Bernoulli r.v. where $\sigma = \sqrt{p(1-p)}$. 
In this case $p(1-p)$ has the \textit{upper bound} $1/4$. We can replace this value in $I$. 
Then $I = [\widehat{p} - q_{\alpha/2} \frac{1}{2\sqrt{n}}, \widehat{p} + q_{\alpha/2} \frac{1}{2\sqrt{n}}]$.\\

\textbf{Plug-in:} Since $\widehat{\theta}$ is a consistent estimator of $\theta$, we can simply replace $\theta$ with $\widehat{\theta}$.

\textbf{Solve the equation:} A more elaborate method is to solve the system of two Inequalities w.r.t $\theta$. 
$$\widehat{\theta} - q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}} \leq \theta \leq \widehat{\theta} + q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}}$$
$$(\theta - \widehat{\theta})^2 \leq q^2_{\alpha/2} \frac{\sigma^2}{n}$$
Finding the roots of the quadratic equation will lead us to $I_{solve} = (\theta_1, \theta_2)$.

\subsection{Chapter 10 Hypothesis Testing}
Let $X$ be a r.v. and let $\chi$ be the range of $X$. We test a hypothesis by finding an appropriate subset of outcomes $R \subset \chi$ called \underline{rejection region}. 
If $X \in R$ we \underline{reject the null hypothesis}, otherwise, we do not reject it. 
The null hypothesis is called $H_0: \theta \in \Theta_0$ and the alternative hypothesis is called $H_1: \theta \in \Theta_1$ and we retain $H_0$ unless there is strong evidence to reject it. 
$\Theta_0 \ and \ \Theta_1$ are disjoint subsets of the sample space. We can make 2 errors when testing a hypothesis:\\

\textbf{Type 1 error:} Rejecting $H_0$ when $H_0$ is true.\\
\textbf{Type 2 error:} Retaining $H_0$ when $H_1$ is true.\\

The \textbf{power function} of a test is defined as $\beta(\theta) = \mathbb{P}_\theta (X \in R)$, and the \textbf{size} of a test is defined to be 
$\alpha = \underset{\theta \in \Theta_0}{max} \beta(\theta)$. The test is then said to have level $\alpha$.


\subsubsection{p--values}
Generally, if the test rejects at level $\alpha$ it will also reject at level $\alpha^{'} > \alpha$. 
Hence, the p--value is the smallest level $\alpha$ at which we can \underline{reject} $H_0$.
Therefore, the smaller the p--value, the stronger the evidence against $H_0$.
\textbf{Example:} We toss a coin 30 times and get 13 heads. We want to test $H_0: \theta = 1/2$ vs. $H_1: \theta \not = 1/2$. By CTL:

$$\sqrt{n} \frac{\overline{X_n} - \theta_{H_0}}{\sigma_{H_0}} \approx 0.77$$
Then the p--value is $\mathbb{P}_\theta (|Z| > 0.77) = 2 \mathbb{P}_\theta(Z < - 0.77) \approx 0.44$

\section{Unit 3 Mehods of Estimation}

\subsection{Distance TV and KL divergence}

The \textbf{total variation} distance between two probabilities is defined as 
$$TV(\mathbb{P}_\theta, \mathbb{P}_{\theta'}) = 1/2 \underset{x \in E}{\sum} |p_\theta (x) - p_{\theta'}(x)|$$

We replace the $\sum$ for an integral in the case of continuous r.v's. The goal is to create an estimator that minimizes this distance, however it's unclear
how to proceed.\\

\textbf{Kullback-Leibler} Also known as the \underline{relative entropy}, mesure the divergence between two probabilities.
$$KL(\mathbb{P}_\theta, \mathbb{P}_{\theta'}) = \underset{x \in E}{\sum} p_\theta(x) log \left(\frac{p_\theta (x)}{p_{\theta^{'}}(x)} \right)$$
Estimating the KL divergence yield the \underline{maximum likelihood principal} presented bellow.

\subsection{Chapter 9 Parametric Inference}
\subsubsection{Maximum Likelihood Estimator}
The likelihood function is defined as $$\mathcal{L}_n(\theta) = \prod_{i=1}^{n} f(X_i; \theta)$$
and the log-likelihood is defined as $l_n(\theta) = log(\mathcal{L}_n(\theta))$.\\
The \textbf{maximum likelihood estimator} is denoted by $\widehat{\theta}_n$
that maximizes $\mathcal{L}_n(\theta)$. Usually we follow theses steps to calculate the MLE of a parameter:
\begin{enumerate}
    \item Calculate $\mathcal{L}_n(\theta)$
    \item Calculate the log-likelihood, $l_n(\theta)$
    \item Derivate w.r.t the parameter, $\theta$, and set the derivative to 0
    \item Solve for $\theta$.
\end{enumerate}

Here's a table  of likelihood function for popular distributions.\\

\begin{tabular}{l c}
    \textit{Distribution} &\textit{Likelihood Function}\\
    Bernoulli & $p^{\sum x_i} (1-p)^{n- \sum x_i}$\\
    Poisson & $(\lambda^{\sum x_i} e^{-n\lambda})/ \prod x_i$\\
    Gaussian & $(1/2 \pi \sigma^2)^{n/2} e^{-(\sum(x_i - \mu)^2)/(2\sigma^2))}$\\
    Exponential & $\lambda^n e^{-\lambda \sum x_i}$\\
    Uni[0, b] & $(1/b^n) \mathbbm{1}(max(x_i) \leq b)$
\end{tabular}
\newline

\textbf{Asymptotic Normality of the MLE:} 
\begin{itemize}
    \item The parameter is identifiable
    \item $\forall \theta \in \Theta$, the support of $\mathbb{P}_\theta$ does not depend on $\theta$
    \item $\theta_\star$ is not on the boundary of $\Theta$, otherwise $l(\theta)$ may not be differentiable
    \item $I(\theta)$ is invertible in the neighborhood of $\theta_\star$
    \item A few other technical conditions
\end{itemize}

Then, $\widehat{\theta}^{MLE}_{n} \overset{P}{\underset{n \to \infty}{\to}} \theta^* \ w.r.t. \ \mathbb{P}_\theta^{*}$ and  
$\sqrt{n}(\widehat{\theta}^{MLE}_n - \theta^*) \overset{d}{\underset{n\to \infty}{\to}} N(0, I(\theta^*)^{-1})$, 
where $I(\theta^*)$ is the \underline{Fisher Information}, defined as $I(\theta) = -\mathbb{E}_\theta[\mathbb{H}l(\theta)] = -\mathbb{E}_\theta[l"(\theta)]$.

\subsubsection{Method of Moments}
Let $X_1, \dots, X_n$ be an i.i.d sample associated with $(E, (\mathbb{P}_\theta)_{\theta \in \Theta})$.\\
$\longrightarrow$ Population moments: $m_k(\theta) = \mathbb{E}_\theta [X_1^k]$\\
$\longrightarrow$ Empirical moments: $ \widehat{m}_k (\theta) = \overline{X}_n^k$\\

\textbf{Example:} We have a normal distribution model, $N(\mu, \sigma^2)$. Given data, the method of moments will estimate the parameters as:
$$\widehat{m}_1 = \widehat{\mu} = \overline{X}_n$$
$$\widehat{m}_2 = \widehat{\mu^2} + \widehat{\sigma^2} = \overline{X^2_n}$$

All that is left is to solve the equations system.

\subsubsection{M-Estimators}

Goal: estimate the parameter $\mu^*$ associated with some unknown distribution $\mathbb{P}$, e.g.: it's mean, median, variance, etc. 
Find some function $\rho: E \times M \to \mathbb{R}$, where $M$ is the set of all possible values of $\mu^*$ such that:
$$\mathcal{Q}(\mu) := \mathbb{E} [\rho(X_1, \mu)]$$
achives its minimum at $\mu = \mu^*$.\\
\textbf{Examples:}\\ If $E = M = \mathbb{R} \text{ and } \rho(x, \mu) = (x - \mu)^2; \forall (x, \mu) \in \mathbb{R}; \mu^* = \mathbb{E}[X]$\\
If $E = M = \mathbb{R} \text{ and } \rho(x, \mu) = |x - \mu|; \forall (x, \mu) \in \mathbb{R}; \mu^*$ is the \underline{median} of $\mathbb{P}$\\

\textit{The empirical median is always more robust than the mean, meaning that it's less affected by missing or wrong data.}

\subsection{All the matrices/vectors}

\textbf{Covariance Matrix($\Sigma$):} formed by the $var(X_i)$ in the diagonals and $Cov(X_i, X_j)$ for the other terms.

$$\Sigma = 
\left[
\begin{array}{ccc}
    var(X_1) & \ldots & cov(X_1, X_d)\\
    \vdots & \vdots & \vdots\\
    cov(X_d, X_1) & \ldots & var(X_d)
\end{array}
\right]_{d \times d}
$$

\textbf{Gradient($\nabla$):} first derivative matrix of a function matrix $g(\theta)$

$$\nabla g(\theta) = 
\left[
\begin{array}{ccc}
    \frac{\partial g_1(\theta)}{\partial \theta_1}& &\frac{\partial g_k(\theta)}{\partial \theta_1}\\
    \vdots& \vdots & \vdots\\
    \frac{\partial g_1(\theta)}{\partial \theta_d}& &\frac{\partial g_k(\theta)}{\partial \theta_d}\\
\end{array}
\right]_{d \times k}
$$

\textbf{Hessian($\mathbb{H}$):} secong derivative matrix of a function matrix $g(\theta)$

$$\mathbb{H} g(\theta) = 
\left[
\begin{array}{ccc}
    \frac{\partial^2 g(\theta)}{\partial \theta_1^2}& &\frac{\partial^2 g(\theta)}{\partial \theta_d \theta_1}\\
    \vdots& \vdots & \vdots\\
    \frac{\partial^2 g(\theta)}{\partial \theta_1 \theta_d}& &\frac{\partial^2 g(\theta)}{\partial \theta_d^2}\\
\end{array}
\right]_{d \times d}
$$
\textit{Note: the function $g(\theta)$ is said to be concave if $x^T \mathbb{H}g(\theta)x \leq 0, \forall x \in \mathbb{R}^d$.}


\textbf{Fisher Information Matrix ($I(\theta)$):} is defined as $-\mathbb{E}_\theta [\mathbb{H}l(\theta)]$

\subsection{Multivariate Distributions}
\textbf{Normal}
$$f(x; \mu, \Sigma) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}} \text{exp}\left \{-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)\right \}$$

\subsection{Huber's Loss}

The Huber's Loss function allow us to render absolute functions differentiable, for example $|x|$, which is not differentiable at 0.

\begin{align*}
h_\delta (a) &=
    \begin{cases}
        a^2/2,                       & |a| \leq \delta\\
        \delta (|a| - \delta /2),    & \text{otherwise}
    \end{cases}\\
h^{'}_\delta (a) &=
    \begin{cases}
        2a,          & |a| \leq \delta\\
        \pm \delta,  & \text{otherwise}
    \end{cases}
\end{align*}

\end{multicols}
\end{document}

