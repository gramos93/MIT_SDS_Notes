\documentclass[landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{enumitem}
\usepackage{bbm}

\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\title{MIT 18.650x Cheatsheet}

\begin{document}

\raggedright
\footnotesize

\begin{center}
    \textbf{MIT 18.650x Mid-Term 1 Cheatsheet}\\
\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{1pt}

\section{Unit 2 - Parametric Estimation and Confidence Intervals}
In this unit, we introduce a mathematical formalization of statistical modeling to make a principled sense of the \underline{Trinity of Statistical} inference.

\begin{enumerate}
    \item {Estimation: $\hat{p} = \overline{R_n}$ is an estimator for the real parameter $p^*$.}
    \item {Confidence intervals: $[0.56, 0.73]$ is a 95\% confidence interval for $p$.}
    \item {Hypothesis testing: "We found \textit{statistical evidence} that more couples turn their head to the right."}
\end{enumerate}

\subsection{Chapter 4 - Inequalities}
Inequalities are used to bound quantities that might otherwise be hard to compute. \\

\textbf{Markov's inequality:} let $X$ be a non-negative r.v. and suppose $\mathbb{E}(X)$ exists. For any $t > 0$,
$$\mathbb{P}(X > t) = \frac{\mathbb{E}[X]}{t}$$

\textbf{Chebyshev's inequality:} let $\mu = \mathbb{E}[X]$ and $\sigma^2 = Var(X)$. Then, $$\mathbb{P}(|X - \mu| \ge t) \leq \frac{\sigma^2}{t^2}$$

\textbf{Hoeffding's inequality:} let $X_1, \dots, X_n \sim{} Ber(p)$ then for any $\epsilon > 0$, $$\mathbb{P}(|\overline{X_n} - p| > \epsilon) \leq 2e^{\frac{-2n\epsilon^2}{(b-a)^2}}$$
Where $\overline{X_n}$ is the sample average and $X \in [a, b]$.\\

\textbf{Jensen's inequality:} If $g$ is convex, then $$\mathbb{E}[g(X)] \ge g(\mathbb{E}[X])$$
The reverse is true if $g$ is concave.

\subsection{Chapter 5 - Convergence of r.v.'s}
There are 2 main ideas in this chapter. First, the \textbf{LLN} says that the \textit{sample average} converges in probability to $\mathbb{E}$. Second, the \textbf{CTL} says that
$\sqrt{n}(\overline{X_n} - \mu)$ converges in distribution to a Normal distribution for large enough $n$.\\
Let $X_1, \dots, X_n$ be a sequence of r.v's, $X$ is another r.v. Let $F_n(t)$ denote the CDF of $X_n$ an $F(t)$ denote the CDF of $X$.\\

\textbf{Convergence in probability:} $X_n \overset{P}{\to} X$ if, for every $\epsilon > 0$, $$\mathbb{P}(|X_n - X| > \epsilon) \to 0$$ as $n \to \infty$. 
\textbf{Warning !} One might think that $X_n \overset{P}{\to} b$ implies $\mathbb{E}[X] \to b$. This is \textbf{not} true. 

\textbf{Convergence in distribution:} $X_n \overset{d}{\to} X$ if, $$\lim_{n \to \infty} F_n(t) = F(t)$$ for all $t$ for which $F$ is continuous. 

\subsubsection{Properties of convergence}
The following relationships hold:
\begin{enumerate}[itemsep=0em]
    \item{$X_n \overset{P}{\to} X$ implies that $X_n \overset{d}{\to} X$.}
    \item{If $X_n \overset{d}{\to} X$ and if $\mathbb{P}(X=c) = 1$ for some real number $c$, then $X_n \overset{P}{\to} X$.}
    \item{If $X_n \overset{P}{\to} X$ and $Y_n \overset{P}{\to} Y$ then, $X_n + Y_n \overset{P}{\to} X+Y$.}
    \item{If $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} c$ then, $X_n + Y_n \overset{c}{\to} X + c$.}
    \item{If $X_n \overset{P}{\to} X$ and $Y_n \overset{P}{\to} Y$ then, $X_n Y_n \overset{P}{\to} XY$.}
    \item{If $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} c$ then, $X_n Y_n \overset{d}{\to} cX$.}
    \item{If $X_n \overset{P}{\to} X$ then, $g(X_n) \overset{P}{\to} g(X)$.}
    \item{If $X_n \overset{d}{\to} X$ then, $g(X_n) \overset{d}{\to} g(X)$.}
\end{enumerate}

Parts (4) and (6) are known as the \textbf{Slutzky's theorem}. Note that $X_n \overset{d}{\to} X$ and $Y_n \overset{d}{\to} Y$ 
does not imply that $X_n = Y_n \overset{d}{\to} X+Y$.

\subsubsection{Weak LLN \& CTL}
\textbf{Interpretation of the WLLN:}The distribution of $\overline{X_n}$ becomes more concentrated around $\mu$ as $n$ get larger. From the Chebyshev's inequality: 
$\mathbb{P}(|\overline{X_n} - \mu| > \epsilon) \leq \frac{Var(\overline{X_n})}{\epsilon^2} = \frac{\sigma^2}{n \epsilon^2}.$

\textbf{Central Limit Theorem:}let $X_1, \dots, X_n$ be i.i.d with mean $\mu$ and variance $\sigma^2$. Then 
$$Z_n \equiv \frac{\overline{X_n} - \mu}{\sqrt{Var(\overline{X_n})}} = \frac{\sqrt{n} (\overline{X_n} - \mu)}{\sigma} \overset{(d)}{\to} Z$$
where $Z$ is a standard Normal.\\
\textbf{Multivariate} version of the CTL. Let $X_1, \dots, X_n$ be i.i.d random \textit{vectors}, $X_i = [X_{1}, X_{2}, \dots, X_{k}]^T$ 
with mean $\mu = [\mu_{1}, \mu_{2}, \dots, \mu_{k}]^T$
and covariance matrix $\Sigma$. Then,$$\sqrt{n}(\overline{X} - \mu) \overset{(d)}{\to} N(0, \Sigma)$$

\subsubsection{The Delta Method}
The Delta Method allows us to find the limiting distribution of $g(Y_n)$ where $g$ is a \underline{smooth} function.
Suppose that $\frac{\sqrt{n}(Y_n - \mu)}{\sigma} \overset{(d)}{\to} N(0, 1)$ and $g$ is a \underline{differentiable} function such that $g'(\mu) \ne 0$. Then,

$$\sqrt{n}(g(Y_n) - g(\mu)) \overset{(d)}{\to} N(0, g'(\mu)|^2 \sigma^2)$$

\textbf{Multivariate} version of the Delta method. Let $Y_n = (Y_{n1}, \dots, Y_{nk})$ be a sequence random \textit{vectors} 
such that $\sqrt{n}(Y_n - \mu) \overset{(d)}{\to} N(0, \Sigma)$.
In this case $g: \mathbb{R}^k \to \mathbb{R}.$ Then,

$$\sqrt{n}(g(Y_n) - g(\mu)) \overset{(d)}{\to} N(0, \nabla g(\mu)^T \Sigma \nabla g(\mu))$$

\subsection{Chapter 6 Statistical Inference}
\textit{"\dots is the process of using data to infer the distribution that generated that data."}\\

\subsubsection{Point estimation: } 

Provides a \textit{best guess} of some quantity of interest. The point estimate of $\theta$, a fixed number, is $\hat{\theta}$, 
which depends on the data, therefore a r.v. A point estimator $\hat{\theta}_n$ of the parameter $\theta$ is \textbf{consistent} if $\hat{\theta}_n \overset{P}{\to} \theta$.
The \textbf{bias} of an estimator is defined as $bias(\hat{\theta}_n) = \mathbb{E_{\theta}}[\hat{\theta}_n] - \theta$. 
An estimator is unbiased if $\mathbb{E_{\theta}}[\hat{\theta}_n] = 0$.
Following the bias, the \textbf{mean squared error} (MSE) is $MSE = bias^2(\hat{\theta}_n) + \mathbb{V_{\theta}}(\hat{\theta}_n)$.\\

\subsubsection{Confidence Intervals: } 
Is an interval $I = (a, b)$ such that $\mathbb{P}_{\theta}(\theta \in I) \ge 1-\alpha$, for all $\theta \in \Theta$. 
$I$ is random, but $\theta$ is not random. Suppose that $\widehat{\theta}_n \approx N(\theta, \sigma^2)$. The confidence interval will be of the form 
$$I = \left [\widehat{\theta}_n - q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}}, \widehat{\theta}_n + q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}} \right ]$$

However, most of the time, this confidence interval will depend on the true parameter $\theta$. There are 3 methods to fix this problem.\\

\textbf{Conservative Bound:} Suppose we're calculating a confidence interval for the parameter $p$ of a Bernoulli r.v. where $\sigma = \sqrt{p(1-p)}$. 
In this case $p(1-p)$ has the \textit{upper bound} $1/4$. We can replace this value in $I$. 
Then $I = [\widehat{p} - q_{\alpha/2} \frac{1}{2\sqrt{n}}, \widehat{p} + q_{\alpha/2} \frac{1}{2\sqrt{n}}]$.\\

\textbf{Plug-in:} Since $\widehat{\theta}$ is a consistent estimator of $\theta$, we can simply replace $\theta$ with $\widehat{\theta}$.

\textbf{Solve the equation:} A more elaborate method is to solve the system of two Inequalities w.r.t $\theta$. 
$$\widehat{\theta} - q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}} \leq \theta \leq \widehat{\theta} + q_{\alpha/2} \frac{\sqrt{\sigma^2}}{\sqrt{n}}$$
$$(\theta - \widehat{\theta})^2 \leq q^2_{\alpha/2} \frac{\sigma^2}{n}$$
Finding the roots of the quadratic equation will lead us to $I_{solve} = (\theta_1, \theta_2)$.

\section{Unit 3 Mehods of Estimation}

\subsection{Distance TV and KL divergence}

The \textbf{total variation} distance between two probabilities is defined as 
$$TV(\mathbb{P}_\theta, \mathbb{P}_{\theta'}) = 1/2 \underset{x \in E}{\sum} |p_\theta (x) - p_{\theta'}(x)|$$

We replace the $\sum$ for an integral in the case of continuous r.v's. The goal is to create an estimator that minimizes this distance, however it's unclear
how to proceed.\\

\textbf{Kullback-Leibler} Also known as the \underline{relative entropy}, mesure the divergence between two probabilities.
$$KL(\mathbb{P}_\theta, \mathbb{P}_{\theta'}) = \underset{x \in E}{\sum} p_\theta(x) log \left(\frac{p_\theta (x)}{p_{\theta^{'}}(x)} \right)$$
Estimating the KL divergence yield the \underline{maximum likelihood principal} presented bellow.

\subsection{Chapter 9 Parametric Inference}
\subsubsection{Maximum Likelihood Estimator}
The likelihood function is defined as $$\mathcal{L}_n(\theta) = \prod_{i=1}^{n} f(X_i; \theta)$$
and the log-likelihood is defined as $l_n(\theta) = log(\mathcal{L}_n(\theta))$.\\
The \textbf{maximum likelihood estimator} is denoted by $\widehat{\theta}_n$
that maximizes $\mathcal{L}_n(\theta)$. Usually we follow theses steps to calculate the MLE of a parameter:
\begin{enumerate}
    \item Calculate $\mathcal{L}_n(\theta)$
    \item Calculate the log-likelihood, $l_n(\theta)$
    \item Derivate w.r.t the parameter, $\theta$, and set the derivative to 0
    \item Solve for $\theta$.
\end{enumerate}

Here's a table  of likelihood function for popular distributions.\\

\begin{tabular}{l c}
    \textit{Distribution} &\textit{Likelihood Function}\\
    Bernoulli & $p^{\sum x_i} (1-p)^{n- \sum x_i}$\\
    Poisson & $(\lambda^{\sum x_i} e^{-n\lambda})/ \prod x_i$\\
    Gaussian & $(1/2 \pi \sigma^2)^{n/2} e^{-(\sum(x_i - \mu)^2)/(2\sigma^2))}$\\
    Exponential & $\lambda^n e^{-\lambda \sum x_i}$\\
    Uni[0, b] & $(1/b^n) \mathbbm{1}(max(x_i)) \leq b)$
\end{tabular}
\newline

\textbf{Asymptotic Normality of the MLE:} 
\begin{itemize}
    \item The parameter is identifiable
    \item $\forall \theta \in \Theta$, the support of $\mathbb{P}_\theta$ does not depend on $\theta$
    \item $\theta_\star$ is not on the boundary of $\Theta$, otherwise $l(\theta)$ may not be differentiable
    \item $I(\theta)$ is invertible in the neighborhood of $\theta_\star$
    \item A few other technical conditions
\end{itemize}

Then, $\widehat{\theta}^{MLE}_{n} \overset{P}{\underset{n \to \infty}{\to}} \theta^* \ w.r.t. \ \mathbb{P}_\theta^{*}$ and  
$\sqrt{n}(\widehat{\theta}^{MLE}_n - \theta^*) \overset{d}{\underset{n\to \infty}{\to}} N(0, I(\theta^*)^{-1})$, 
where $I(\theta^*)$ is the \underline{Fisher Information}, defined as $I(\theta) = -\mathbb{E}_\theta[\mathbb{H}l(\theta)] = -\mathbb{E}_\theta[l"(\theta)]$.

\subsubsection{Method of Moments}
Let $X_1, \dots, X_n$ be an i.i.d sample associated with $(E, (\mathbb{P}_\theta)_{\theta \in \Theta})$.\\
$\longrightarrow$ Population moments: $m_k(\theta) = \mathbb{E}_\theta [X_1^k]$\\
$\longrightarrow$ Empirical moments: $ \widehat{m}_k (\theta) = \overline{X}_n^k$\\

\textbf{Example:} We have a normal distribution model, $N(\mu, \sigma^2)$. Given data, the method of moments will estimate the parameters as:
$$\widehat{m}_1 = \widehat{\mu} = \overline{X}_n$$
$$\widehat{m}_2 = \widehat{\mu^2} + \widehat{\sigma^2} = \overline{X^2_n}$$

All that is left is to solve the equations system.

\subsubsection{M-Estimators}

Goal: estimate the parameter $\mu^*$ associated with some unknown distribution $\mathbb{P}$, e.g.: it's mean, median, variance, etc. 
Find some function $\rho: E \times M \to \mathbb{R}$, where $M$ is the set of all possible values of $\mu^*$ such that:
$$\mathcal{Q}(\mu) := \mathbb{E} [\rho(X_1, \mu)]$$
achives its minimum at $\mu = \mu^*$.\\
\textbf{Examples:}\\ If $E = M = \mathbb{R} \text{ and } \rho(x, \mu) = (x - \mu)^2; \forall (x, \mu) \in \mathbb{R}; \mu^* = \mathbb{E}[X]$\\
If $E = M = \mathbb{R} \text{ and } \rho(x, \mu) = |x - \mu|; \forall (x, \mu) \in \mathbb{R}; \mu^*$ is the \underline{median} of $\mathbb{P}$\\

\textit{The empirical median is always more robust than the mean, meaning that it's less affected by missing or wrong data.}

\subsection{All the matrices/vectors}

\textbf{Covariance Matrix($\Sigma$):} formed by the $var(X_i)$ in the diagonals and $Cov(X_i, X_j)$ for the other terms.

$$\Sigma = 
\left[
\begin{array}{ccc}
    var(X_1) & \ldots & cov(X_1, X_d)\\
    \vdots & \vdots & \vdots\\
    cov(X_d, X_1) & \ldots & var(X_d)
\end{array}
\right]_{d \times d}
$$

\textbf{Gradient($\nabla$):} first derivative matrix of a function matrix $g(\theta)$

$$\nabla g(\theta) = 
\left[
\begin{array}{ccc}
    \frac{\partial g_1(\theta)}{\partial \theta_1}& &\frac{\partial g_k(\theta)}{\partial \theta_1}\\
    \vdots& \vdots & \vdots\\
    \frac{\partial g_1(\theta)}{\partial \theta_d}& &\frac{\partial g_k(\theta)}{\partial \theta_d}\\
\end{array}
\right]_{d \times k}
$$

\textbf{Hessian($\mathbb{H}$):} secong derivative matrix of a function matrix $g(\theta)$

$$\mathbb{H} g(\theta) = 
\left[
\begin{array}{ccc}
    \frac{\partial^2 g(\theta)}{\partial \theta_1^2}& &\frac{\partial^2 g(\theta)}{\partial \theta_d \theta_1}\\
    \vdots& \vdots & \vdots\\
    \frac{\partial^2 g(\theta)}{\partial \theta_1 \theta_d}& &\frac{\partial^2 g(\theta)}{\partial \theta_d^2}\\
\end{array}
\right]_{d \times d} 
$$
\textit{Note: the function $g(\theta)$ is said to be concave if $x^T \mathbb{H}g(\theta)x \leq 0, \forall x \in \mathbb{R}^d$.}


\textbf{Fisher Information Matrix ($I(\theta)$):} is defined as $-\mathbb{E}_\theta [\mathbb{H}l(\theta)]$

\subsection{Multivariate Distributions}
\textbf{Normal}
$$f(x; \mu, \Sigma) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}} \text{exp}\left \{-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)\right \}$$

\subsection{Huber's Loss}

The Huber's Loss function allow us to render absolute functions differentiable, for example $|x|$, which is not differentiable at 0.

\begin{align*}
h_\delta (a) &=
    \begin{cases}
        a^2/2,                       & |a| \leq \delta\\
        \delta (|a| - \delta /2),    & \text{otherwise}
    \end{cases}\\
h^{'}_\delta (a) &=
    \begin{cases}
        2a,          & |a| \leq \delta\\
        \pm \delta,  & \text{otherwise}
    \end{cases}
\end{align*}


\section{Unit 4 Hypothesis Testing}
\subsection{Chapter 10}

Let $X$ be a r.v. and let $\chi$ be the range of $X$. We test a hypothesis by finding an appropriate subset of outcomes $R \subset \chi$ called \underline{rejection region}. 
If $X \in R$ we \underline{reject the null hypothesis}, otherwise, we do not reject it. 
The null hypothesis is called $H_0: \theta \in \Theta_0$ and the alternative hypothesis is called 
$H_1: \theta \in \Theta_1$ and we retain $H_0$ unless there is strong evidence to reject it. 
$\Theta_0 \ and \ \Theta_1$ are disjoint subsets of the sample space. We can make 2 errors when testing a hypothesis:\\

\textbf{Type 1 error:} Rejecting $H_0$ when $H_0$ is true.\\
\textbf{Type 2 error:} Retaining $H_0$ when $H_1$ is true.\\

The \textbf{power function} of a test is defined as $\beta(\theta) = \mathbb{P}_\theta (X \in R)$, and the \textbf{size} of a test is defined to be 
$\alpha = \underset{\theta \in \Theta_0}{max} \beta(\theta)$. The test is then said to have level $\alpha$.

\subsubsection{Wald's Test}

Consider testting $H_0: \theta = \theta_0 \text{ vs. } H_1: \theta \ne \theta_0$
Assuming that $\widehat{\theta}$ is asymptotically Normal we have :

$$W = \frac{(\widehat{\theta} - \theta_0)}{\widehat{se}}, \text{ where } \widehat{se} = \sqrt{\sigma^{2}_{0}/n}$$

The size $\alpha$ Wald test will reject $H_0$ when $|W| > z_{\alpha/2}$.\\
\textit{Note: in the case of a two samples test (e.g.: comparing two means), $\widehat{se} = \sqrt{(\sigma^{2}_{1}/n) + (\sigma^{2}_{2}/m)}$, where m and n are the sample sizes.}

\subsubsection{p--values}

Generally, if the test rejects at level $\alpha$ it will also reject at level $\alpha^{'} > \alpha$. 
Hence, the p--value is the smallest level $\alpha$ at which we can \underline{reject} $H_0$.
Therefore, the smaller the p--value, the stronger the evidence against $H_0$.
\textbf{Example:} We toss a coin 30 times and get 13 heads. We want to test $H_0: \theta = 1/2$ vs. $H_1: \theta \not = 1/2$. By CTL:

$$\sqrt{n} \frac{\overline{X_n} - \theta_{H_0}}{\sigma_{H_0}} \approx 0.77$$
Then the p--value is $\mathbb{P}_\theta (|Z| > 0.77) = 2 \mathbb{P}_\theta(Z < - 0.77) \approx 0.44$

\subsubsection{The \texorpdfstring{$\chi^2$}{TEXT} Distribution}

The $\chi^2$ distribution is defined as $V = \sum_{i = 1}^{k} Z^{2}_{i}$, where the $Z_i$ are standard normal r.v.'s.
We say that $V$ has $k$ degrees of freedom, which coincides with the $\mathbb{E}[V]$, while the $var(V) = k^2$. The PDF of V is:

$$f(v) = \frac{v^{k/2 - 1} e^{-v/2}}{2^{k/2} \Gamma(k/2)}, \text{ for } v > 0$$

\textbf{Cochran's Theorem}

For $X_1,\dots, X_n \sim  N(\mu, \sigma^2)$, then $\overline{X_n} {\perp \!\!\! \perp} S_n, \: \forall n$, where $S_n$ is the sample variance. 
Furthermore, $\frac{n S_n}{\sigma^2} \sim \chi^{2}_{n-1}$. We often prefer the unbiased variance estimator $\tilde{S_n} = \frac{n S_n}{n-1}$.

\subsubsection{Student-T Distribution}

The Student-T distribution with $d$ degrees of freedom, denoted $t_d$, is the law of r.v.'s that follow 

$$\frac{Z}{\sqrt{V/d}},$$ where $Z \sim N(0, 1), \: V \sim \chi^{2}_{d}$ and $Z {\perp \!\!\! \perp} V$.\\

From the student-T distribution, we derive the \textbf{t-Test}. Specially \underline{usefull when the sample sizes are small} and one can not make use of the 
large numbers theorems (CTL, Slutsky's, etc.).\\

Let $X_1,\dots, X_n \sim  N(\mu, \sigma^2)$, with unknown parameters. We want to test $H_0: \mu = 0 \text{ vs. } H_1: \mu \ne 0$. Our test statistic is :

$$T_n = \frac{\overline{X_n} - \mu}{\sqrt{\tilde{S_n}/n}} = \sqrt{n} \frac{\overline{X_n}/\sigma}{\sqrt{\tilde{S_n}/\sigma^2}} \sim \frac{N(0, 1)}{\chi^{2}_n-1 / (n-1)}$$

Since the numerator of the equation above $\sim N(0, 1)$, under $H_0$, and the denominator $\sim \chi^{2}_n-1 / (n-1)$ they are independent by Cochran's theorem.
So finally we have a Student-T distribution $T_n \sim t_{n-1}$, and the test with \underline{non-asymptotic} level $\alpha$ is 
$\psi = \mathbbm{1}(|T_n| > q_{\alpha/2})$ where $q_{\alpha/2}$ is the $\alpha/2$ quantile of $t_{n-1}$.\\
In the case of a two samples T-test Tn has the following form: 

$$T_n = \frac{\overline{X_n} - \overline{Y_m} - (\mu_x - \mu_y)}{\sqrt{(\sigma^{2}_{x}/n) + (\sigma^{2}_{y}/m)}} \sim t_N$$
where $N$ is given by the Welch-Satterthwaite formula :

$$N = \frac{(\sigma^{2}_{x}/n + \sigma^{2}_{y}/m)^2}{\frac{\sigma_{x}^{4}}{n^2(n-1)} + \frac{\sigma_{y}^{4}}{m^2(m-1)}} \geq min(n, m)$$

Some advantages and drawbacks of the T-test are:
\begin{itemize}[itemsep=0em]
    \item Non asymptotic
    \item Can be run on small samples sizes
    \item Can also be run on large sample sizes
    \item Assumes the data is Gaussian
\end{itemize}

\subsubsection{MLE based test}

Let $\widehat{\theta}^{MLE}$ be the MLE of $\theta$ and assume that it's technical conditions are satisfied. From the Wald's test we have under $H_0$ :

$$\sqrt{n}\: I(\theta)^{1/2} (\widehat{\theta}^{MLE} - \theta_0) \overset{(d)}{\underset{n\rightarrow \infty}{\rightarrow}} N_d(0, I_d)$$

The goal of the test is to mesure the Eucledian distance between two vectors, we end up with : 

$$T_n = n\: (\widehat{\theta}^{MLE} - \theta_0)^T \: I(\widehat{\theta}^{MLE}) \: (\widehat{\theta}^{MLE} - \theta_0) \rightarrow \chi^{2}_{d}$$

\textit{Note: The Wald's test is still valid if the test is one sided, however it's less powerfull.}

\subsubsection{Log-Likelihood Test}

The likelihood test is usefull for testing vector-valued parameters. Consider testing $H_0: \theta \in \Theta_0 \text{ versus } H_1: \theta \not\in \Theta_0$.
The test  statistic is :

$$T_n = 2(l_n(\widehat{\theta_n}) - l_n(\widehat{\theta_0})) \overset{(d)}{\underset{n\rightarrow \infty}{\rightarrow}} \chi^{2}_{r - q}$$

Here $\theta$ is of the form $(\theta_1, \dots, \theta_q, \theta_{q+1}, \dots, \theta_r)$ and the parameter space 
$\Theta_0 = \{ \theta: (\theta_{q+1}, \dots, \theta_r) = (\theta_{q+1}^{(0)}, \dots, \theta_r^{(0)})\}$. Both $\widehat{\theta}$'s are MLE's, but $\widehat{\theta_0}$
is constrained to the parameter space $\Theta_0$. In english, we're testing whether a subset of the parameter vector $\theta$ equals the vector $\Theta_0$. 
For example, testing whether $\theta_4=\theta_5=0 \text{ in } \theta = (\theta_1, \dots, \theta_{5})$.

\subsection{Goodness of Fit Tests}

GOF's tests are used to check if the data come from an assumed parametric model.  For example if the data is Gaussian, uniform or comes from a multinomial distribution.\\

\subsubsection{Chi-Squared Test - Discrete distributions}

The multinomial likelihood is $L_n(X_1, \dots, X_n; \vec{p}) = p_{1}^{N_1}p_{2}^{N_2} \dots p_{k}^{N_k}$, where $N_k$ is the number of times that $x = X_i$. 
The MLE is $\widehat{p}_j = N_j / n$ and $\sqrt{n}(\widehat{\vec{p}} - \vec{p_0})$ is asymptotically normal under $H_0$. Therefore the following theorem holds true :

$$T_n = n \sum^{K}_{j=1} \frac{(\widehat{p}_j - p_{j}^{0})^2}{p_{j}^{0}} \overset{(d)}{\underset{n\rightarrow \infty}{\rightarrow}} \chi^{2}_{K-1}$$

\textit{Note: this test \underline{does not} work for continuous distribution. For example, to test a normal distribution we would need to "bin" our data and test
the probability of falling under on of the bins. To fully analyse continuous distributions we need the \underline{CDF and the Empirical CDF}.}\\

\subsubsection{Empirical CDF}

Let $X_1,\dots, X_n$ be real r.v., the CDF is defined as $F(t) = \mathbb{P}[X_i \leq t]\: \forall t \in \mathbb{R}$. Now the empirical CDF (a.k.a sample CDF) is defined as :

$$F_n(t) = \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}(X_i \leq t) \: \forall t \in \mathbb{R}$$

By the strong law of large numbers $F_n(t) \overset{a.s.}{\underset{n\rightarrow \infty}{\rightarrow}} F(t)$, but this is only a \underline{pointwise} convergence.
To have a \emph{uniform convergence} of the function we need the Glivenko-Canteli theorem (also known as the fundamental theorem of statistics), defined as :

$$\lim_{n \rightarrow \infty} \underset{t\in \mathbb{R}}{sup} \: |F_n(t) - F(t)| \overset{a.s.}{\underset{n\rightarrow \infty}{\rightarrow}} 0$$

By the CTL, $\sqrt{n} (F_n(t) - F(t)) \overset{(d)}{\underset{n\rightarrow \infty}{\rightarrow}} N(0, \sigma^2)$ and since the empirical CDF is a indicator functio,
it's asymptotic variance is $F_n(t)(1-F_n(t))$. Once again, the goal is to mesure the distance between two functions.

\subsubsection{Kolmogorov-Smirnov Test}

Considering $H_0: F = F_0 \text{ v.s } H_1: F \neq F_0$, the KS test is defined as :

$$T_n = \underset{t\in \mathbb{R}}{sup} \: \sqrt{n}|F_n(t) - F(t)_0| \overset{(d)}{\underset{n\rightarrow \infty}{\rightarrow}} \underset{0 \leq t \leq 1}{sup} |\mathbb{B}(t)|$$
where $\mathbb{B}(t)$ is a Brownian bridge distribution (Donker's Theorem), which is a pivotal distribution and it's quantiles can be obtained in tables. The Ks test with asymptotic level $\alpha$
is $\delta^{KS}_{\alpha} = \mathbbm{1}(T_n > q_\alpha)$, where $q_\alpha$ is the (1-$\alpha$) of sup $|\mathbb{B}(t)|$.\\

In real life, we compute $T_n$ as follows :

\begin{enumerate}[itemsep=0em]
    \item $F_0$ is non-decreasing function, $F_n$ is piecewise constant, with jumps at $ t_i = X_i, \: i=1, \dots, n$.
    \item Let $X$ be the \emph{reordered} sample.
    \item The expression for $T_n$ reduces to the following pratical formula: 
    $$\sqrt{n} \underset{i=1,\dots, n}{max} \left\{ max \left( \left| \frac{i -1}{n} - F_0(X_i) \right|, \left| \frac{i}{n} - F_0(X_i) \right| \right) \right\}$$
    \item Under $H_0, \: T_n$ is a \emph{pivotal statistic} and does not depend on the distribution of the $X_i$'s but it depends on $n$. 
    It is easy to reproduce in simulations and find the values in KS tables.
\end{enumerate}

Other GOFs test exist such as the Cramer-Von Mises ($L_2$ distance) and the Anderson-Darling tests.

\subsubsection{Kolmogorov-Lillofors Test}

Now, if we want to check if our data is Gaussin and we use a plug-in estimators for $\mu \text{ and } \sigma^2$ in the KS test, Donker's theorem \emph{is no longer valid}. 
Instead, we compute que quantiles for the test statistic: $$T_n = \underset{t\in \mathbb{R}}{sup} \: \sqrt{n}|F_n(t) - \psi_{\widehat{\mu}, \widehat{\sigma^2}}(t)|$$
This test statistic does not depend on the unknown parameters and it's quantiles can be obtained in K-L tables for different values of $n$.

\section{Unit 5}
\subsection{Bayesian Inference}

In the Bayesian approach we use our belief or prior knowlage of what the parameter vector $\theta$ might be. Here's how it is carried out :

\begin{enumerate}[itemsep=0em]
    \item We choose a \emph{prior distribution} that expresses our beliefs about $\theta$ before seeing the data.
    \item We define the a statistical model, $f(x|\theta)$ that reflects our data given our parameter.
    \item After observing the data we update our belief and calculate a \emph{posterior distribution} $f(\theta| X_1, \dots, X_n)$.
\end{enumerate}

\subsubsection{Bayes' Method}

From Bayes' Theorem we can compute the prior distribution as follows :

$$\pi(\theta | X^n) = \frac{\mathcal{L}(X^n|\theta) \pi(\theta)}{f(X^n)} \propto \mathcal{L}(X^n|\theta) \pi(\theta)$$

In the formula above, $f(X^n)$ is a \emph{normalizing constant} and it can be removed from the equation since it does not depend on $\theta$.
The posterior is the said to be \emph{proportional} to the likelihood times (or weigthed by) the prior. Since the posterior distribution has to 
integrate to 1 on it's sample space, we can always recover the constant later if needed.

\subsubsection{Bayes' Interval Estimate}

One can estimate a Bayesian interval by finding $a$ and $b$ such that 
$\int_{-\infty}^{a} f(\theta|x^n)d\theta = \int^{\infty}_{b} f(\theta|x^n)d\theta = \alpha / 2$. Let $C = (a, b)$, then :

$$\mathbb{P}(\theta \in C|x^n) \int_{b}^{a} f(\theta|x^n)d\theta = 1 - \alpha$$

so $C$ is a $1 - \alpha$ \emph{posterior interval}.

\subsubsection{Improper Priors \& "Noninformative" Priors}

If we have no information about the parameter, how to pick a prior ? A good candidate is the \emph{constant prior}, for example $\pi(\theta) \propto 1$.
If the parameter space $\Theta$ is bounded ([0, 1]), then this is juste the \emph{uniform prior} on $\Theta$. However, if $\Theta$ is not bounded, then this is not a 
proper prior since it doesn't integrate to 1. We define an \emph{improper prior} as a mesurable, nonnegative function $\pi(.)$ defined on $\Theta$ that is not
integrable. However we can still define a posterior with an improper prior.

\subsubsection{Conjugate Priors}

When the posterior distribution  belongs to the same distribution family as the prior distribution, the prior is called a \emph{conjugate prior} to the likelihood model.
Here're some common conjugate priors: and models :

\begin{tabular}{l l c}
    \textit{Prior} & \textit{Model} & \textit{Posterior Parameters}\\
    Beta    & Bernoulli     & $(\alpha + \sum x_i, \beta + n - \sum x_i)$\\
    Beta    & Binomial      & $(\alpha + \sum x_i, \beta + N_i - \sum x_i)$\\
    Beta    & Geometric     & $(\alpha + n, \beta + \sum x_i)$\\
    Gamma   & Poisson       & $(\alpha + \sum x_i, \beta + n)$\\
    Gamma   & Exponential   & $(\alpha + n, \beta + \sum x_i)$\\
    Normal  & Normal        & See Wikipedia table.
\end{tabular}

\subsubsection{Jeffreys' Prior}

Jeffrey created a rule to choose priors and that is as follows : 

$$\pi_{J}(\theta) \propto \sqrt{det(I(\theta)}$$

Since a \emph{higher} Fisher Information is associated with an \emph{easier point to estimate} (less randomness in the data) than one with lower Fisher Information,
we're trying to balance that by using it as a prior in the Bayes approach. The effect is that we're putting more weight to the points that are easier to estimate.\\
The Jeffreys' prior satisfies a \emph{representation invariance principal}.If $\eta = \psi(\theta)$ in an invertible function then the PDF 
$\tilde{\pi}(.) \text{ of } \eta \text{ satisfies } \pi(\eta) \propto \sqrt{\tilde{I}(\eta)}$. We can write $\tilde{\pi}(\eta)$ in term of $\phi^{'}(.) \text{ and } \phi^{-1}(.)$:

$$\tilde{\pi}(\eta) = \frac{\pi(\phi^{-1}(\eta))}{|\phi^{'}(\phi^{-1}(\eta))|}$$

Some applications of this property would be :

\begin{itemize}
    \item Compute the prior for a $Ber(q^{10})$ instead of $Ber(p)$
    \item Compute the prior for $Exp (1 / \lambda)$ instead of $Exp(\lambda)$
\end{itemize}

\subsubsection{Bayesian Estimation}

The Bayes approach can also be used to estimate the true underlined parameter, however in a frequentist manner. One would start bu calculating the posterior, 
nd then the posterior \emph{mean, meadian or mode} can be used as estimator. Another popular choice is to use the Maximum A Posteriori (MAP) estimator.

$$\widehat{\theta}^{MAP} = \underset{\theta \in \Theta}{argmax} \: \pi(\theta | X_1, \dots, X_n)$$

\section{Unit 6 - Regression}
\subsection{Linear Regression}

Regression is a method for studying the relationship between a \emph{response} variable $Y$ and a \emph{covariate} variable $X$ (often called a feature is the ML world).
One important fact about regression is that it exhibits \emph{correlations}, \textbf{not} \emph{casuality}. 
Here we summarize this relationship via the \emph{regression function}:

$$\mu(x) = \mathbb{E}[Y | X = x] = \int y f(y|x) dy$$

The goal is to estimate the regression function from the data of the form $(X_1, Y_1), \dots, (X_i, Y_i) \sim F_{Y,X}$. 
When we assume $\mu(x)$ is linear this is called \emph{linear regression} and it's simplest form is $\mu(x) = \beta_0 + \beta_1 x$.
To estimate the regression function we need some modeling assumptions:

\begin{itemize}[itemsep=0em]
    \item $(X_i, Y_i)$ are i.i.d from some unknown joint distribution;
    \item $var(X) > 0$ and $var(Y | X = x) > 0$;
    \item This distribution is best described by $h(y|x) = \frac{h(x, y)}{h(x)}$.\\
          It contains all the information of $Y$ given $X$.
\end{itemize}

From above it's clear that if $var(Y | X = x) > 0$, the points do not follow a perfect line. Therefore we introduce a \emph{noise} ($\epsilon$) term to the 
theorical linear regression function, which gives $Y = a^* + b^* x + \epsilon$. 

\subsection{Least Squares Estimator(LSE)}

The LSE are the values of $\widehat{\beta}_0$ and $\widehat{\beta}_1$ that minimizes the \emph{residual sums of squares} or $RSS = \sum_{i = 1}^{n} \widehat{\epsilon}_i^2$.
Where the residuals, $\widehat{\epsilon}_i$ are the vertical distance between the regression function and the $Y_i$'s. We can also use the following form:

$$\sum_{i = 1}^{n} (Y_i - \widehat{\beta}_0 - \widehat{\beta}_1 X_i)^2$$

The LSE's are given by:

$$\widehat{\beta}_1 = \frac{\overline{XY} - \overline{X} \overline{Y}}{\overline{X^2} - \overline{X}^2}$$

$$\widehat{\beta}_0 = \overline{Y_n} - \widehat{\beta}_1 \overline{X_n}$$

When we add the assumption that $\epsilon \sim N(0, \sigma^2)$ given $X,\: Y_i|X_i \sim N(\mu_i, \sigma^2)$. 
Under assumption of \emph{Normality}, the LSE is the same as the MLE where $\widehat{\sigma^2} = \frac{1}{n - p} \sum_{i = 1}^{n} \widehat{\epsilon}_i^2$
is an unbiased estimator of the $\sigma^2$.

\emph{Note: one can use Goodness of Fit to test whether the residuals are Gaussian}.

\subsubsection{Multivariate Regression and Matrix form}

The covariates may come in a vector form and we'll en up with a $\mathbb{X}_{(n \times p)}$ \emph{design matrix} of covariates samples. 
The regression function still works in a multi dimentional space and is of the form:

$$\mathbf{Y}_i = \mathbb{X}_i^T \mathbf{\beta} + \mathbf{\epsilon_i}, \text{ and satisfies }$$
$$\widehat{\mathbf{\beta}} = \underset{\mathbf{\beta} \in \mathbb{R}^{p}}{argmax} ||\mathbf{Y} - \mathbb{X}^T \mathbf{\beta}||^2_2$$

Assuming that $rank(\mathbb{X}) = p$, which means that $\mathbb{X}$ is invertable, we have a closed form solution for the LSE as 
$\widehat{\mathbf{\beta}} = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T \mathbf{Y}$. 

The \emph{geometric} interpretation of the LSE: $\mathbb{X} \mathbf{\beta}$ is the  orthogonal projection of $\mathbf{Y}$ onto the subspace
spanned by the columns of $\mathbb{X}$.


\subsubsection{LSE Inference and Testing}
However, to make further inference we need to make additional assumptions:

\begin{itemize}[itemsep=0em]
    \item The design matrix $\mathbb{X}$ is \textbf{deterministic} and invertible
    \item The model is \emph{homoscedastic}: $\varepsilon_i$ are i.i.d
    \item The noise vector is Gaussian $N(0, \sigma^2)$, which leads to $Y_i | X_i \sim N(\mathbb{X}\mathbf(\beta)^*, \sigma^2 I_p)$
\end{itemize}

\emph{Note : if $\mathbb{X}$ is not deterministic, all the above can be understood \textbf{conditionally} on $\mathbb{X}$}.

Theses assumptions give rise to the following properties:

\begin{itemize}[itemsep=0em]
    \item LSE = MLE
    \item $\widehat{\mathbf{\beta}} \sim N(\mathbf{\beta}^*, \sigma^2 (\mathbb{X}^T \mathbb{X})^{-1})$
    \item Quadratic risk of $\widehat{\mathbf{\beta}}$ is $\sigma^2 \, tr((\mathbb{X}^T \mathbb{X})^{-1})$
    \item The prediction error is $\sigma^2 (n - p)$
\end{itemize}

By Cochran's theorem, $\mathbf{\widehat{\beta}} {\perp \!\!\! \perp} \widehat{\sigma}^2$, therefore we can use a modified T-test to test the significancy of the 
j-th explanatory variable in the linear regression. For example, let $H_0: \beta_j = 0 \text{ vs } H_1: \beta_j \ne 0$.

$$ T_{n}^{(j)} = \left| \frac{\widehat{\beta}_j - \beta_j}{\sqrt{\sigma^2 \gamma_j}} \right| \sim t_{n-p}$$

where $\gamma_j$ is the j-th \emph{diagonal} element is the $(\mathbb{X}^T \mathbb{X})^{-1}$ matrix. The test with non asymptotic level $\alpha$ is 
$\psi_{j} = \{ T_n^{(j)} > q_{\alpha/2}(t_{n-p}) \}$. In the case where we're testing multiple explanatory variable et use the \textbf{Bonferroni's Test}.
Let $H_0: \mathbf{\beta}_j = 0 \text{ vs } H_1: \mathbf{\beta}_j \ne 0, \: (\forall j \in S)$, where $S \subseteq \{1, \dots, p\}$. To have non asymptotic level $\alpha$
one has to use $\alpha / k$, where $k = |S|$ (number of tests).

\end{multicols}
\end{document}